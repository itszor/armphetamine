\chapter{Preparation}

\section{What do we want?}

Starting back at the premise that we want to make \arm\ emulation faster on \ia\ PCs, there might be various approaches we could take. The first might be to code a `traditional' instruction interpreter in assembly language rather than a higher-level language like C -- this is commonly done\footnote{An ARM710 emulator for the \ia\ processor exists \cite{3DO} (not publically available at the time of writing), forming part of a 3DO game-console emulation project.}. However, due to the unavoidable amount of work which must be duplicated each time an instruction is decoded and executed with this approach, we would never begin to approach the speed of the host machine.

Instead, if we can somehow translate the binary code of emulated programs into the native code format before we run them, we might be able to boost their execution speed up to somewhere closer to that of the host machine. How is this possible? I will discuss two approaches which could be taken.

\subsection{Static recompilation}

The first approach is static translation of program binaries. Suppose we can statically filter program binaries for one system to obtain binaries which will run on another system. If the only difference between the two systems is the processor type with all other things being equal (as in Windows NT for Alpha vs \ia), this approach might be feasible.

Consider some problems with this approach, even in an ideal situation where we have a richly-annotated binary format which specifies exactly what is code and what is data. A primary problem would be handling execution flow control in the target code, even in simple cases as opposed to just more complex ones. For example, an instruction typically used to return from a subroutine in \arm\ code looks like this:

\begin{code}
movs pc,r14
\end{code}

The return address (the instruction immediately following the subroutine call) is copied into the register {\tt r14} before the call, and {\tt pc}, alias {\tt r15}, can be treated like a normal register. The instruction shown returns after the subroutine call by simply moving the return address in {\tt r14} into the program counter. In more complex cases, the program counter can be used as the destination register for any arbitrary computation.

One problem which arises is due to the different address spaces we're considering, for the original code and the target code we're producing. For this type of instruction to be translatable at all, we would need some form of mapping table along with our translated binary specifying where the new \ia\ code equivalent of every original \arm\ instruction lies. This would cause a huge memory overhead in itself, but additionally each piece of \ia\ code we generate must be `atomic', and not rely on any state from previous instructions. This requirement would probably lead to unacceptably huge, inefficient code.

Complex program analysis techniques might help alleviate this problem. Another problem which might be encountered though involves constructs such as self-modifying code, dynamically-linked libraries, and so on. These would require that the recompiler, or at least an ordinary instruction emulator, would have to be present whenever the binary was executed to handle any code not dealt with in the original recompilation phase.

The most serious problem arises when we remove the detailed information about code and data areas (which will not be present in all cases in real-world code) -- it isn't realistically possible to deduce what is code and what is data by statically analysing arbitrary chunks of memory. Consider a fragment of code which might be used for branch-table indirection (eg, {\tt switch} statement) for example:

\begin{code}
adds~pc,pc,r0,lsl~\#2~~; pc $\leftarrow$ pc+(r0<<2)\\
mov~r0,r0~~~~~~~~~~~~~; nop\\
bl~firstcase~~~~~~~~~~; executed when r0=0\\
bl~secondcase~~~~~~~~~; executed when r0=1\\
bl~thirdcase~~~~~~~~~~; ...\\
...
\end{code}

Theoretically, the first statement in this fragment of code may branch anywhere in memory. Without performing extremely complex, detailed analysis of the entire source code to determine every possible value of {\tt r0} at that point in the program (which is likely to be impossible, and at best is probably a `hard' AI problem), we cannot proceed any further. Program analysis techniques dealing with branch-table code like this, as well as code generated by language features like function pointers, are the subject of ongoing research in decompilation techniques \cite{Mycroft99}.

The final reason for not attempting this kind of translation is that I wanted to be able to eventually emulate a complete \arm-based computer. This includes not only application binaries, but also low-level operating system routines such as device drivers, interrupt handlers, and so on. The static recompilation model cannot meet this sort of requirement.

\subsection{Dynamic recompilation}

The second approach is to recompile code dynamically, rather than statically. That means fragments of code are translated a bit at a time, {\it as code is being emulated}, rather than beforehand in one sweep.

Dynamic recompilation has a `traditional' emulator at its core, which reads the guest system's instructions one at a time and interprets them. As this is done, we make a note of any sections which are executed often, then we take those sections and translate them into native code. In this way, we sidestep the problem of statically determining where code lies and what is data. The `difficult' cases like the branch table described above can simply be delegated to the traditional emulator.

When the system is initialised, all code will be executed using the traditional emulator, but once it has been running for a while virtually all of the emulated program will be running natively -- hopefully at a speed approaching that of native code.

There are still many problems with this approach which need careful consideration. How is it possible to translate code from one processor type to another? What do we do with the two processors' internal registers, status flags, memory maps, modes and system call implementations? What about keeping track of control flow between two fundamentally different types of code?

\section{What needs to be done}

At an early stage, the project was seen to be formed from two seperate parts, a traditional emulator for \arm\ code and a recompiler capable of transforming \arm\ code into equivalent \ia\ code and invoking it. In a sense, the \arm\ code can be looked at as a low-level `high-level' language, and the two parts of the project as an interpreter and compiler for that language. I will present the preparations made for these two parts of the project in turn.

\section{Interpretive emulation}

This might perhaps seem trivial, just a case of breaking down \arm\ instructions and updating the state of a virtual processor accordingly. Source code to various existing emulators was examined (\cite{Dorr9X}, \cite{Gilbert9X}), and the instruction format for the \arm\ was studied \cite{ARM94}. All ARM instructions are squeezed into 32 bits, so this format is necessarily fairly compact, and decoding it efficiently in software isn't quite as straightforward as it might be. I will discuss this further in the next chapter.

\section{Recompiling emulation}

\subsection{Understanding the problem}

Of fundamental importance to the approach taken in translation is the degree of similarity between the source and target codes. This was investigated thoroughly at an instruction-set level before any possible implementations were even considered (\cite{HELPPC}, \cite{Intel99}). In some senses, the two processors were found to be similar: for instance, both have 32-bit data processing operations and both use status flags for testing conditions and controlling program flow. The operations available in the \ia\ ISA are, for the most part, an (unclean) superset of those in the \arm\ ISA - that is, both machines can add, subtract, multiply, do logical AND and OR operations, and so on. Unfortunately, the similarities end soon after that.

\subsection{Breaking it down}

It was clear even at this early stage that translation between two types of code was going to involve more than simply filtering instructions one at a time. There were at least three problems which must be considered: register allocation, flag coherency and control flow. Additionally, \arm\ instructions can actually be fairly sophisticated, combining operations which would require several instructions on some other processors into one. Take as a quick example:

\begin{code}
~~~~~~~~addlts r0,r1,r2,asl \#7
\end{code}

If hand-translated, the simplest \ia\ code you might expect from this might look something like this:

\begin{code}
~~~~~~~~jge~skip~~~~~~~~~~; conditional execution\\
~~~~~~~~movl~\%ecx,\%eax~~~~; r0 $\leftarrow$ r2\\
~~~~~~~~shll~\$7,\%eax~~~~~~; r0 $\leftarrow$ r0 << \#7\\
~~~~~~~~addl~\%ebx,\%eax~~~~; r0 $\leftarrow$ r0 + r1\\
skip:~~~...
\end{code}

That's assuming the processor condition-code flags make sense at the time of the jump instruction, that the \arm\ registers {\tt r1,r2} have already been loaded into \ia\ registers {\tt \%ebx,\%ecx}, and that the {\tt \%eax} register was free for use as the destination register {\tt r0}.

Notice how there are three seperate work-performing instructions: the jump, the shift, and the addition. These can easily be seperated out into different, much simpler instructions, which will have more easily-deduced \ia\ counterparts, will be much simpler to analyse, and (naturally) can be shared by different types of \arm\ instruction as needed. It was decided that a form of register-transfer language was to be used as an intermediate code, the exact form of which will be discussed in the next chapter.

\subsection{Compilation}

Now, what was needed was an algorithm for transforming intermediate code into \ia\ code. The Dragon Book \cite{Dragon9X} was very useful in finding out about various compilation techniques.

It was felt that some sort of rule-based system for matching sections of intermediate code to sections of \ia\ code was desirable, rather than attempting to hard-code one-to-one transformations into the recompiler. A form of tree-matching algorithm was designed, discussed further in the next chapter.

\subsubsection{Cache coherency}

One possible failing point of the emulator -- that of coherency between instruction and data caches on the \ia\ processor -- was tested long before work on the emulator proper started. On many RISC processors, special effort must be made to ensure previously-cached memory locations are flushed from the internal instruction cache after self-modifying code -- this is often a slow process, and could become a severe hindrance in a project of this sort. These early tests proved to be successful -- fortunately, the \ia\ processor architecture (for compatibility reasons) requires that the instruction cache is able to `snoop' on the data cache to ensure that coherency problems of this sort don't occur. The test also served to ensure that there would be no permission problems in running code from dynamically-allocated store, which could again have complicated matters.

\section{Implementation}

\subsection{Choice of platform}

It was chosen to use the C language using the GCC compiler under the Linux operating system for development purposes. This afforded the use of free, high quality tools throughout -- the GCC compiler itself, GNU Make and the CVS version control system among many others.

An Acorn Risc PC running NetBSD was also available, which was useful for writing (and verifying the operation of) test binaries using a real ARM processor.

\subsection{Software engineering}

For a project of this size, keeping code well-structured starts to become important. Use of global variables should be kept to a minimum, and code should be split into different modules where possible.

\subsection{Backups}

Backups of both source and documentation were to be kept on both development machines, on Cambridge University's archiving service, Pelican, and in the available personal filespace on the Lab's Thor machine.

\section{Project title}

Following in a long tradition of bad puns based on the name of the ARM processor, the working title of this project is ``ARMphetamine'' -- a speedy ARM emulator. I apologise profusely for this.
