\chapter{Evaluation}

\section{Current status}

The subset of an \arm\ processor tackled is emulated correctly under interpretive emulation, to the extent that complex test programs like BBC BASIC will run as intended. The rest of the project, comprising profiler, intermediate code translator etc., performing the far more difficult task of rewriting \arm\ machine code as \ia\ machine code, emulates code correctly the vast majority of the time -- consider that a single-bit error caused by one instruction in ten thousand can cause execution of a program to fail. The Dhrystone 2.1 benchmark has been successfully run using this mode of emulation.

\subsection{Interpreter}

Interpretive emulation (with faked system calls) is accurate enough to run a copy of ARM BBC BASIC extracted from Acorn's RISC OS 3.1 operating system. Hand-coded in assembly language by the designer of the \arm\ ISA, this piece of code seems to use just about every permutation of available features on every instruction, and was taken to be a good `tough' test for emulation: more so than code generated by a compiler, for example.

There are some limitations in the interpretive emulator, but no more than were anticipated and planned for at the start of the project for the sake of simplicity.

\subsection{System call emulation}

A subset of RISC OS system calls are supported at an emulator-code level, to handle simple I/O (in a Unix terminal) and supply enough environment information to allow BBC BASIC to run. This method for dealing with I/O was found to be ideal for testing purposes.

\subsection{Profiler}

The profiler suffices to gather information on programs given the limited subset of the \arm\ being emulated. However, issues would begin to arise if the emulator reached the point where it could be used to power a complete system. Recompiled code is never discarded, so after a while memory will be exhausted. My intended scheme to resolve this was to allocate a fixed circular buffer cache for recompiled code sequences (discarding those at the beginning when the buffer became full). Unfortunately, these chunks may have complex interdependencies due to the {\tt xjmp} control flow construct, which means a more sophisticated mechanism must be employed, possibly involving lists of dependencies and a ``pending removal'' flag for each chunk. Memory leaks could still become a serious issue if emulation ran for an extended period of time.

There is currently no support for self-modifying code.

\subsection{Recompiler}

The recompiler (comprising phetacode translator, code analyser, pattern matcher, runtime assembler, code generator, register allocator, flag cacher, etc.) works for many simple example programs. I can give some proof of this -- see the division example of appendix C. Recompilation is accurate enough to be able to run the ``Dhrystone'' benchmark program (version 2.1, written in C).

Unfortunately, it seems that minor bugs remain which currently prevent more complex test programs from working fully (BBC BASIC currently fails after a fashion, for instance).

\subsubsection{A simple example}

Here, I give a small example of successful translation from \arm\ code to \ia\ code. This program is written to encourage the recompiler to process one simple chunk of code then stop:

\begin{verbatim}
        mov r1,#5
recomploop:
        bl countdown
        subs r1,r1,#1
        bge recomploop

stop:   b stop

countdown:
        mov r0,#100
decrloop:
        subs r0,r0,#1
        bge decrloop

        mov pc,r14
\end{verbatim}

The first part of this code calls a subroutine, {\tt countdown}, just enough times to convince the recompiler to translate it, then stops\footnote{Zero-offset branches are trapped for debugging purposes, and act as `stop emulation now' commands.}. The translation of {\tt countdown} it produces looks like this:

\begin{verbatim}
   0:	movl $0x64,%eax       ; = mov r0,#100
   6:	subl $0x1,%eax        ; = subs r0,r0,#1
   c:	cmc                   ; invert carry
   d:	setb 0x58(%ebp)       ; store carry flag
  11:	seto 0x59(%ebp)       ; store overflow flag
  15:	sete 0x5a(%ebp)       ; store zero flag
  19:	sets 0x5b(%ebp)       ; store negative flag
  1d:	jge  0x6              ; = bge decrloop
  23:	movl 0x38(%ebp),%ebx  ; = mov pc,r14
  26:	addl $0x8,%ebx        ; pc += pipeline offset
  2c:	andl $0x3fffffc,%ebx  ; mask pc address bits
  32:	movl %eax,0x0(%ebp)   ; store r0
  35:	movl %ebx,0x3c(%ebp)  ; store pc
  38:	ret    
\end{verbatim}

Hopefully its fairly clear that this code will do the same thing on both processors (note {\tt [b|j]ge} isn't dependent on the state of the carry flag on either processor). The generated code clearly isn't perfect -- the storage of condition codes (addresses {\tt 0xc}-{\tt 0x19}) is too eager in the {\tt subs}, {\tt bge} loop, for example.

\subsubsection{A more complex example -- division}

See Appendix C for an example of a complete translation of a division routine coded in \arm\ assembly language through phetacode into \ia\ assembly language.

\subsection{Benchmarks}

For benchmarking purposes, the standard ``Dhrystone'' test was used\footnote{Dhrystone 2.1 as compiled for NetBSD/arm32 won't run unmodified in the simplified execution environment of the emulator -- some modifications had to be made to allow I/O to function adequately. These changes were not in the main body of the benchmark and won't have affected the results.}. The table in figure \ref{benchmark} shows the results for various machines.

\begin{figure}[tbh]

\begin{center}
\begin{tabular}{llll}
Processor [recomp] & Compiler/OS & Platform & Dhry/sec\\
\hline
ARMphetamine [off] & GCC/NetBSD & Linux/x86 & 5527\\
12MHz ARM250 & Norcroft/RISC OS & RISC OS/arm26 & 6169\\
ARMphetamine [on] & GCC/NetBSD & Linux/x86 & 34843\\
233MHz StrongARM & GCC/NetBSD & NetBSD/arm32 & 275482\\
450MHz AMD K6-2 & GCC/Linux & Linux/x86 & 823045
\end{tabular}
\end{center}

\caption{\label{benchmark}Dhrystone benchmark results}
\end{figure}

The ARM250 and StrongARM results are running natively on real ARM-series processors. ARMphetamine itself was running on an AMD K6-2 Linux/x86 machine, whose own Dhrystone result is given for comparison purposes.

The sixfold increase in speed for ARMphetamine with recompilation enabled rather than disabled is slightly disappointing -- it was hoped that performance up to an order of magnitude better than that might be achieved. Bear in mind though that firstly, Dhrystone is a synthetic benchmark and so not necessarily representative of real code, and secondly, not much effort has been expended at this point in trying to optimise any part of the emulator. I expect that if more work were done in this area, speed could be substantially improved (the original recompiled Dhrystone result, prior to a few simple improvements, was around 19000). Profiling the code with {\tt gprof} shows that, behind running chunks of recompiled code, most time is spent reading and writing words to emulated store, profiling emulated code and looking up hash-table entries. These functions would obviously be the first canditates for optimisation or rewriting in assembly language should increases in speed be desired.

\section{Debugging}

This was not an easy program to debug. Aside from being fairly big (currently over ten thousand lines of C), there were several intrinsic problems in debugging various parts of the program, which I will describe below.

\subsection{Sourcing ARM binaries}

To create ARM test binaries, a machine running NetBSD/arm32 was available, networked to the main development machine running Linux/x86. I wanted to be able to compile and run C test programs on the NetBSD machine, then take the same binaries and run them under emulation, comparing the results.

This strategy hit problems early on. I recognized the need to emulate a subset of NetBSD system calls to provide simple I/O, but I underestimated exactly how much of the operating system has to be present even to get as far as the {\tt main()} function in compiled C code. An unsatisfactory solution was originally found, loading unlinked ``.o'' files, and emulating a couple of NetBSD calls (including simplified versions of {\tt read} and {\tt write} for I/O). The arm32 port of NetBSD doesn't appear to be very well-documented, so even to get this far required fairly extensive trawling of the kernel source. And even then, test programs had to be written in assembly language.

When the interpreter reached a certain level of maturity and I wanted to be able to run more complex test programs on it\footnote{i.e., the ARM BBC BASIC interpreter.}, the entire NetBSD system call faking mechanism was scrapped, and a new RISC OS system call faking mechanism was created (Edwin Dorr's emulator \cite{Dorr9X} was used as a reference here).

Now, test binaries could no longer be compiled and run on the NetBSD machine. This limitation was worked around by writing RISC OS-savvy I/O routines using GCC/arm32's inline assembler, and conditionally compiling them into test binaries depending on whether they were intended for running natively or under emulation. A new workaround for the complexities surrounding {\tt main()} was found, which conditionally compiles a dummy {\tt main()} and renames the real main function to {\tt fake()}. This allows statically-linked binaries (eg, Dhrystone) from NetBSD to run on the emulator, albeit with a little modification.

\subsection{The interpreter}

Most of the time in the ARM instruction interpreter, as in any other interpreter, a fairly tight loop of instructions is being executed and the only visible effects are the alteration of a set of variables. In this situation, a standard debugger such as {\tt gdb} is of limited use. It's possible to hard-code `watchpoints' and breakpoints of sorts into the emulator source, working with a disassembly of the program being emulated.

Once the interpreter was working to a certain extent, the only strategy for finding obscure bugs which was found to work reliably was extremely tedious -- printing out traces of instructions being executed followed by the values in every register and the processor flags. In some `difficult' cases like\footnote{Read as ``load r5 from the address pointed to by r5+16, writing back the calculated address into r5''}:

\begin{code}
ldr~r5,[r5,\#16]!
\end{code}

it was found that the technical documentation available didn't actually specify what value {\tt r5} should contain after the operation. To actually find that a single instruction like this is causing an obscure problem with the high-level operation of the code (a similar instruction was present in the BBC BASIC module) is quite tricky. Another instruction:

\begin{code}
movs~r0,\#256
\end{code}

caused a great deal of puzzlement: the value of {\tt r0} was not used afterwards, and in fact the only purpose of the instruction was its non-obvious side-effect of clearing the carry flag\footnote{Typically, the {\tt s} suffix on a {\tt mov} instruction would cause only the zero and negative flags to be set. The carry flag is set because the barrel shifter is used to decode the value of the immediate 256, which is stored in the instruction encoding as an eight-bit mantissa and a rightwards rotation by an even number of bits. The documentation available didn't mention this curiosity.}. The high-level manifestation of failing to do this correctly caused, within emulated BBC BASIC, behaviour like this:

\begin{code}
>PRINT~1/0.1\\
~~~~~~~~30
\end{code}

To find this bug, and others like it, BASIC itself had to be disassembled and examined in relation to line-by-line disassembly of execution traces from emulation runs exhibiting the incorrect behaviour.

Perhaps half the bugs in the interpreter were found in this ``top-down'' way. The other half were found using a ``bottom-up'' approach of carefully cross-referencing the operation of each instruction with the ARM documentation.

\subsection{The recompiler}

\subsubsection{Phetacode translator}

The phetacode translator, being similar in spirit to the instruction interpreter described above, was fairly easy to write and debug. It was tested mostly with fragments of one or two instructions of code to begin with, and its output checked by hand. As the code is fairly simple and bears quite a strong relation to the original \arm\ code, this verification process was fairly trivial. Features such as branch-destination backpatching were slightly complicated by the fact that the buffer that phetacode instructions are written into occasionally needs to be extended and may move around in memory when it does so.

\subsubsection{\ia\ code generation}

There isn't a great deal I can say about the process of implementing and debugging the \ia\ code generation algorithm. The register allocation was quite tricky to get right, and was rewritten from scratch at one point. The pattern-matching algorithm for fitting translation rules onto chunks of phetacode was also fairly tricky. Both now seem to be working satisfactorily.

It should be stressed that the way that the recompiler scales from simple fragments of code to larger programs, especially where many cross-linked chunks are involved, can get immensely complex. It often isn't easy to tell where a program is failing, and it's not really feasable to trace the flow of execution of emulated code when control is continually jumping between two types of processor. Again, {\tt gdb} was of limited use\footnote{The authors of Executor claim to have a modified version of {\tt gdb} able to disassemble both the native \ia\ code and emulated m68k code -- something like this might have been useful, but was beyond my means to create for this project.}. This means debugging becomes a case of:

\begin{itemize}

\item Attempting to discover which chunk of recompiled code is at fault
\item Cross-referencing that code back to its equivalent phetacode
\item Occasionally, cross-referencing that phetacode back to the original \arm\ code

\end{itemize}

The first of these can be eased by limiting code which can be recompiled to a restricted range of addresses. This can be useful technique for narrowing down faultily-translated chunks -- if you're lucky -- but becomes less useful when the problem stems from the interaction of multiple chunks, as it frequently did.

The second can be aided slightly by inserting {\tt nop} instructions in generated \ia\ code -- this doesn't help a great deal, but at least it becomes possible to tell where the boundaries are between fragments of code output by different functions.

\section{Quality of compiled code}

All things considered, the quality of compiled code is actually pretty good. Leaving the values of registers in memory whenever possible means that registers aren't spilled nearly as often as you might expect, and the ability to combine phetacode instructions means that, in places at least, fairly tight \ia\ code can be produced. Needful to say, the generated code does actually work most of the time, too!

There are various improvements which could be made, however. There are a couple of relatively cheap optimisations which could be performed -- for example, branch and copy propagation -- which could improve the quality of code a fair amount. Other optimisation techniques used by normal compilers could probably also be adopted beneficially.

Memory access seems to be a huge bottleneck at the moment, so it would be good to alleviate this somehow -- perhaps inlining memory accesses rather than calling out to a C function might help, but this could cause unacceptable code blow-up if software MMU emulation was added.

The flag caching mechanism isn't quite as good as it could be. It's unfortunate that the \ia\ makes it so difficult to alter individual condition code flags (except the carry flag, which has its own manipulation instructions), but there is an alternative scheme which might be worthy of consideration. The idea would be to move away from the low-level one-to-one mapping of \arm\ flags and use a higher-level approach based on a small (eight-byte maximum) ``predicate cache'' and a little data-flow analysis. At the moment, only four variants on the \ia\ {\tt set} instruction are used corresponding directly to four flags, but the \ia\ actually provides a full complement of similar instructions, including a subset corresponding to the same condition codes available for ARM branches.

So, where a conditional instruction (branch) is encountered, the intermediate code representation would be scanned backwards until the last flag-affecting instruction (or instructions) were found. Then, the \ia\ {\tt set} instruction corresponding to the condition on the branch would be inserted directly afterwards in a fixed location in the predicate cache. Where it is found that more than one predicate is attached to a particular condition-setting instruction, more than one {\tt set} instruction can be inserted. For example, code meaning this:

\begin{code}
cmpl~\%eax,\%ebx~~~~$\leftarrow$~this~condition\\
addl~\%ecx,\%edx~~~~$\leftarrow$~corrupted~here\\
jae~foo~~~~~~~~~~~$\leftarrow$~...but~needed~here
\end{code}

would be written like this:

\begin{code}
cmpl~\%eax,\%ebx\\
setae~aeoffset(\%ebp)\\
addl~\%ecx,\%edx\\
testb~\$1,aeoffset(\%ebp)\\
jnz~foo
\end{code}

As you may appreciate, this is much better code than would be produced by the current system, which would require a verbose flag-restore sequence before the branch, but could only be produced at the expense of more sophisticated data-flow analysis, probably straddling multiple basic blocks. Care must be taken, of course, that the true values of the \arm\ flags are recovered at the end of each chunk.

Even better, it may occasionally be possible to reorder instructions in such a way as to completely eliminate the need to store the condition code. The above code could be transformed into:

\begin{code}
addl~\%ecx,\%edx\\
cmpl~\%eax,\%ebx\\
jae~foo
\end{code}

To do this may require a more malleable intermediate representation than the current phetacode. Whether this sort of optimisation would make much difference to the performance of the emulator is perhaps debatable -- the trade-off between compilation and execution time would depend to some extent on the program being emulated (although currently recompilation time seems to be negligible).

%\subsection{Proof of correctness}

%Correctness of code translation has not been explored in a particularly deep or methodical way. Some benefit may be gained by obtaining a formal proof that no transformations performed on \arm\ code in the process of converting it to \ia\ code affect its meaning, even if only for debugging purposes.

%It's fair to say that such a proof, generated either manually or automatically, is beyond the scope of this project.

\section{Software engineering}

The implementation of this project comprises over ten thousand lines of C source code. Good coding practices are vital to prevent unmanagability of a project of this size. Use of {\tt cvs} meant that it wasn't necessary to retain large sections of commented-out code on the offchance it might be needed again, and use of {\tt autoconf} and {\tt automake} meant that Makefile maintainance was largely automatic.

\subsection{Modularity}

The code is split into around 25 modules, shown in figure \ref{modules}. To a large extent, these are independent of each other or only dependent on each other in a logical fashion. Use of global variables is kept to an absolute minimum.

\begin{figure}
\begin{center}
\begin{tabular}{ll}
Module name & Purpose\\
\hline
allocate.c & Register allocation\\
analyse.c & Phetacode analysis\\
block.c & ARM execution profiling\\
cnew.c & Simple memory allocation\\
codegen.c & Code generation\\
decode3.c & ARM instruction decoding/dispatching\\
disassemble.c & ARM disassembly\\
execute.c & ARM instruction interpretation\\
fakesys.c & System call handling\\
generators.c & \ia\ code generators\\
hash.c & Generic hash table handling functions\\
list.c & Generic linked-list handling functions\\
loadaout.c & NetBSD a.out binary loader (bogus)\\
machine.c & Creating and initialising virtual machine\\
main.c & Initialisation\\
memory.c & Memory initialisation, mapping functions\\
nativesupport.c & Invocation of and support functions for native code\\
pqueue.c & Generic priority queue handling functions\\
processor.c & Processor initialisation\\
pseudo.c & ARM $\rightarrow$ phetacode translation\\
pseudodism.c & Phetacode disassembly\\
registers.c & Register file creation and initialisation\\
riscos.c & RISC OS system call faking\\
x86asm.c & Runtime \ia\ assembly\\
x86dism.c & \ia\ disassembly (invokes external program)
\end{tabular}
\end{center}
\caption{\label{modules}Code modules used in the project}
\end{figure}

The language C was found to be ideally suited to the low-level nature of this project.

%An aside: as far as a program of this nature (with a very specific system dependency) can be portable, the code isn't too bad. If recompilation is disabled, the emulator runs flawlessly on NetBSD/arm32, though running on a Linux/PPC machine fails due to the different endianness not being taken into account.

\subsection{Backups}

Tarballs of the CVS repository were regularly copied from the main development machine to Thor and to the secondary development machine. Fortunately, nothing terrible happened to the original copy of the code, so no occasion was found to make use of this backup strategy.
